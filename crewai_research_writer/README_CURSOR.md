# CrewAI プロジェクト - Cursor運用ガイド

このプロジェクトは、Cursorの対話から直接実行できるように設計されています。

## 🚀 クイックスタート

### 方法1: コマンドライン引数で実行（推奨）

```bash
cd /workspace/06_Project/crewai_research_writer
python3 main.py "40代の男性がチャレンジすべき、農業ビジネスのフロンティア"
```

### 方法2: 実行スクリプトを使用

```bash
cd /workspace/06_Project/crewai_research_writer
./run.sh "40代の男性がチャレンジすべき、農業ビジネスのフロンティア"
```

### 方法3: デフォルトトピックで実行

```bash
cd /workspace/06_Project/crewai_research_writer
python3 main.py
# または
./run.sh
```

## 📋 実行フロー

1. **CEO方針決定** - トピックを解釈し、組織全体の方向性を決定
2. **PM計画策定** - プロジェクト計画と品質管理基準を設定
3. **戦略策定** - ソーシャルインパクトの方向性を定義
4. **調査** - トピックに関する情報収集（検索APIが利用可能な場合）
5. **企画立案** - 調査結果を基に革新的な企画を提案
6. **執筆** - すべての成果物を統合した記事を作成
7. **CEO最終レビュー** - 全成果物をレビューし、最終承認

## ⚙️ 設定

### APIキー不要で実行可能

- **LLM**: Ollama（ローカルLLM）を使用
- **検索**: Serper APIキーがなくても動作（事前定義された知識を使用）

### 必要な環境

- Python 3.10以上
- Ollama（自動インストール済み）
- llama3.2モデル（自動ダウンロード済み）

## 📁 出力

生成されたレポートは `output/` ディレクトリに保存されます。

ファイル名形式: `report_YYYYMMDD_HHMMSS_sequential_トピック名.md`

## 🔧 トラブルシューティング

### Ollamaが起動していない場合

```bash
ollama serve &
```

### モデルがダウンロードされていない場合

```bash
ollama pull llama3.2
```

### 実行がタイムアウトする場合

ローカルLLMは処理に時間がかかります。バックグラウンドで実行するか、より小さなモデルを使用してください。

## 💡 Cursorでの使用例

Cursorのチャットから以下のように実行できます：

```
プロジェクトを実行してください。テーマは「40代の男性がチャレンジすべき、農業ビジネスのフロンティア」です。
```

または、ターミナルコマンドとして：

```bash
cd /workspace/06_Project/crewai_research_writer && python3 main.py "40代の男性がチャレンジすべき、農業ビジネスのフロンティア"
```
